{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30ce64c5-d746-4ff4-9f16-d4abff69e6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T10:24:21.240983Z",
     "iopub.status.busy": "2021-11-17T10:24:21.240655Z",
     "iopub.status.idle": "2021-11-17T10:24:21.271865Z",
     "shell.execute_reply": "2021-11-17T10:24:21.271030Z",
     "shell.execute_reply.started": "2021-11-17T10:24:21.240945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "# import spinup.algos.pytorch.ppo.core as core\n",
    "# from spinup.utils.logx import EpochLogger\n",
    "# from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "# from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n",
    "\n",
    "\n",
    "\n",
    "def ppo(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0, \n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01, logger_kwargs=dict(), save_freq=10):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (by clipping), \n",
    "    with early stopping based on approximate KL\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: The constructor method for a PyTorch Module with a \n",
    "            ``step`` method, an ``act`` method, a ``pi`` module, and a ``v`` \n",
    "            module. The ``step`` method should accept a batch of observations \n",
    "            and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``a``        (batch, act_dim)  | Numpy array of actions for each \n",
    "                                           | observation.\n",
    "            ``v``        (batch,)          | Numpy array of value estimates\n",
    "                                           | for the provided observations.\n",
    "            ``logp_a``   (batch,)          | Numpy array of log probs for the\n",
    "                                           | actions in ``a``.\n",
    "            ===========  ================  ======================================\n",
    "            The ``act`` method behaves the same as ``step`` but only returns ``a``.\n",
    "            The ``pi`` module's forward call should accept a batch of \n",
    "            observations and optionally a batch of actions, and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       N/A               | Torch Distribution object, containing\n",
    "                                           | a batch of distributions describing\n",
    "                                           | the policy for the provided observations.\n",
    "            ``logp_a``   (batch,)          | Optional (only returned if batch of\n",
    "                                           | actions is given). Tensor containing \n",
    "                                           | the log probability, according to \n",
    "                                           | the policy, of the provided actions.\n",
    "                                           | If actions not given, will contain\n",
    "                                           | ``None``.\n",
    "            ===========  ================  ======================================\n",
    "            The ``v`` module's forward call should accept a batch of observations\n",
    "            and return:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``v``        (batch,)          | Tensor containing the value estimates\n",
    "                                           | for the provided observations. (Critical: \n",
    "                                           | make sure to flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n",
    "            you provided to PPO.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs of interaction (equivalent to\n",
    "            number of policy updates) to perform.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        clip_ratio (float): Hyperparameter for clipping in the policy objective.\n",
    "            Roughly: how far can the new policy go from the old policy while \n",
    "            still profiting (improving the objective function)? The new policy \n",
    "            can still go farther than the clip_ratio says, but it doesn't help\n",
    "            on the objective anymore. (Usually small, 0.1 to 0.3.) Typically\n",
    "            denoted by :math:`\\epsilon`. \n",
    "        pi_lr (float): Learning rate for policy optimizer.\n",
    "        vf_lr (float): Learning rate for value function optimizer.\n",
    "        train_pi_iters (int): Maximum number of gradient descent steps to take \n",
    "            on policy loss per epoch. (Early stopping may cause optimizer\n",
    "            to take fewer than this.)\n",
    "        train_v_iters (int): Number of gradient descent steps to take on \n",
    "            value function per epoch.\n",
    "        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
    "            close to 1.)\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        target_kl (float): Roughly what KL divergence we think is appropriate\n",
    "            between new and old policies after an update. This will get used \n",
    "            for early stopping. (Usually small, 0.01 or 0.05.)\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Special function to avoid certain slowdowns from PyTorch + MPI combo.\n",
    "#     setup_pytorch_for_mpi()\n",
    "\n",
    "    # Set up logger and save configuration\n",
    "#     logger = EpochLogger(**logger_kwargs)\n",
    "#     logger.save_config(locals())\n",
    "\n",
    "    # Random seed\n",
    "#     seed += 10000 * proc_id()\n",
    "    seed = 0\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "\n",
    "    # Sync params across processes\n",
    "#     sync_params(ac)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "#     logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = steps_per_epoch\n",
    "    buf = PPOBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "#     logger.setup_pytorch_saver(ac)\n",
    "\n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = np.mean(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "#                 logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "#             mpi_avg_grads(ac.pi)    # average grads across MPI processes\n",
    "            pi_optimizer.step()\n",
    "\n",
    "#         logger.store(StopIter=i)\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "#             mpi_avg_grads(ac.v)    # average grads across MPI processes\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "        print(pi_l_old, v_l_old, (loss_pi.item() - pi_l_old), (loss_v.item() - v_l_old))\n",
    "#         logger.store(LossPi=pi_l_old, LossV=v_l_old,\n",
    "#                      KL=kl, Entropy=ent, ClipFrac=cf,\n",
    "#                      DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "#                      DeltaLossV=(loss_v.item() - v_l_old))\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        ep_rewards = []\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v, logp)\n",
    "#             logger.store(VVals=v)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "#                     logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                    ep_rewards.append(ep_ret)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "#         if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "#             logger.save_state({'env': env}, None)\n",
    "        print('Rewards:', np.mean(ep_rewards), np.std(ep_rewards))\n",
    "        # Perform PPO update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "#         logger.log_tabular('Epoch', epoch)\n",
    "#         logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "#         logger.log_tabular('EpLen', average_only=True)\n",
    "#         logger.log_tabular('VVals', with_min_and_max=True)\n",
    "#         logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "#         logger.log_tabular('LossPi', average_only=True)\n",
    "#         logger.log_tabular('LossV', average_only=True)\n",
    "#         logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "#         logger.log_tabular('DeltaLossV', average_only=True)\n",
    "#         logger.log_tabular('Entropy', average_only=True)\n",
    "#         logger.log_tabular('KL', average_only=True)\n",
    "#         logger.log_tabular('ClipFrac', average_only=True)\n",
    "#         logger.log_tabular('StopIter', average_only=True)\n",
    "#         logger.log_tabular('Time', time.time()-start_time)\n",
    "#         logger.dump_tabular()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "#     parser.add_argument('--hid', type=int, default=64)\n",
    "#     parser.add_argument('--l', type=int, default=2)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "#     parser.add_argument('--cpu', type=int, default=4)\n",
    "#     parser.add_argument('--steps', type=int, default=4000)\n",
    "#     parser.add_argument('--epochs', type=int, default=50)\n",
    "#     parser.add_argument('--exp_name', type=str, default='ppo')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "# #     mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "\n",
    "#     from spinup.utils.run_utils import setup_logger_kwargs\n",
    "#     logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "\n",
    "#     ppo(lambda : gym.make(args.env), actor_critic=MLPActorCritic,\n",
    "#         ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, \n",
    "#         seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,\n",
    "#         logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e4a508-6abb-4b5d-8f62-774d403399d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T10:08:20.997832Z",
     "iopub.status.busy": "2021-11-17T10:08:20.997597Z",
     "iopub.status.idle": "2021-11-17T10:08:21.004515Z",
     "shell.execute_reply": "2021-11-17T10:08:21.003963Z",
     "shell.execute_reply.started": "2021-11-17T10:08:20.997813Z"
    }
   },
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "from gym import make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d1a10f-dfea-4b61-96f2-072efbfd95df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T10:06:21.559780Z",
     "iopub.status.busy": "2021-11-17T10:06:21.559478Z",
     "iopub.status.idle": "2021-11-17T10:06:21.576503Z",
     "shell.execute_reply": "2021-11-17T10:06:21.575922Z",
     "shell.execute_reply.started": "2021-11-17T10:06:21.559758Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class MLPCategoricalActor(Actor):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "\n",
    "class MLPGaussianActor(Actor):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, observation_space, action_space, \n",
    "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        if isinstance(action_space, Box):\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)\n",
    "\n",
    "        # build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6f596-901a-4355-82f8-6e8bde8e3b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa98c11d-1a2d-40f5-8cfe-d8e1d99d5cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T10:24:23.484090Z",
     "iopub.status.busy": "2021-11-17T10:24:23.483810Z",
     "iopub.status.idle": "2021-11-17T10:49:40.707492Z",
     "shell.execute_reply": "2021-11-17T10:49:40.704770Z",
     "shell.execute_reply.started": "2021-11-17T10:24:23.484064Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 16 steps.\n",
      "Rewards: 20.973461551662975 4.8336626850168045\n",
      "5.702972316612431e-08 166.59634399414062 -0.015970639172195433 -112.68448638916016\n",
      "Warning: trajectory cut off by epoch at 14 steps.\n",
      "Rewards: 22.980092776503724 6.325435501019595\n",
      "4.1198731537406275e-08 75.49976348876953 -0.01802243128121006 -16.164512634277344\n",
      "Warning: trajectory cut off by epoch at 51 steps.\n",
      "Rewards: 24.232629169246234 7.625512082584904\n",
      "-1.8787384092888715e-08 70.77487182617188 -0.006416627044975698 -32.8084831237793\n",
      "Warning: trajectory cut off by epoch at 10 steps.\n",
      "Rewards: 29.402692202841013 11.426124386672562\n",
      "5.2452087118126656e-08 83.11815643310547 -0.020213689953088476 -31.321353912353516\n",
      "Rewards: 33.57643400423105 12.687781318052668\n",
      "2.956390332542469e-08 65.24459838867188 -0.0182198097407813 -15.232059478759766\n",
      "Warning: trajectory cut off by epoch at 57 steps.\n",
      "Rewards: 37.57687737768194 14.846794837382452\n",
      "2.322197012460947e-08 75.20614624023438 -0.00954732665270619 -12.21368408203125\n",
      "Warning: trajectory cut off by epoch at 30 steps.\n",
      "Rewards: 47.73081072252889 22.08486279477167\n",
      "-1.4305114426349519e-08 131.11813354492188 -0.016923092901707015 -18.972755432128906\n",
      "Warning: trajectory cut off by epoch at 76 steps.\n",
      "Rewards: 54.933788848598994 24.50835684878109\n",
      "8.01086397217432e-09 136.109619140625 -0.013007936628162575 -31.06061553955078\n",
      "Warning: trajectory cut off by epoch at 129 steps.\n",
      "Rewards: 57.166827963132214 25.151886540368455\n",
      "1.0871887390351276e-08 122.90937805175781 -0.014688175798952763 -27.36078643798828\n",
      "Warning: trajectory cut off by epoch at 9 steps.\n",
      "Rewards: 54.853486645165546 24.450932194933284\n",
      "7.820129432900558e-09 100.60174560546875 -0.01565565019547943 -19.89459228515625\n",
      "Warning: trajectory cut off by epoch at 52 steps.\n",
      "Rewards: 64.11989956984063 26.030781385772865\n",
      "-3.8909913513407446e-08 80.83837890625 -0.014556125943361309 -17.945640563964844\n",
      "Warning: trajectory cut off by epoch at 22 steps.\n",
      "Rewards: 62.52388573041658 22.202761264869753\n",
      "-1.0490417423625331e-08 75.95967864990234 -0.016053116634488163 -15.4659423828125\n",
      "Warning: trajectory cut off by epoch at 64 steps.\n",
      "Rewards: 70.751633841751 29.19266505998228\n",
      "-3.051757735406113e-09 97.89124298095703 -0.010657752606272775 -25.17888641357422\n",
      "Warning: trajectory cut off by epoch at 35 steps.\n",
      "Rewards: 72.6554646640047 25.82760017852652\n",
      "7.343292196537732e-09 91.53752899169922 -0.013999021504819353 -24.629348754882812\n",
      "Warning: trajectory cut off by epoch at 200 steps.\n",
      "Rewards: 74.73894632188819 27.381174403658523\n",
      "-1.7356873271978657e-08 78.35169982910156 -0.014264132234453442 -12.16717529296875\n",
      "Warning: trajectory cut off by epoch at 25 steps.\n",
      "Rewards: 77.86278048420847 23.838915815252196\n",
      "-6.38961772381208e-09 61.115875244140625 -0.005012883468717533 -8.730548858642578\n",
      "Warning: trajectory cut off by epoch at 73 steps.\n",
      "Rewards: 73.88439383353987 23.21458219025523\n",
      "-1.602172794434864e-08 68.91926574707031 -0.017330943417549705 -13.281265258789062\n",
      "Warning: trajectory cut off by epoch at 60 steps.\n",
      "Rewards: 75.06152705678463 24.307842891820112\n",
      "5.435943695175638e-09 60.743080139160156 -0.011756734770536514 -15.311553955078125\n",
      "Warning: trajectory cut off by epoch at 60 steps.\n",
      "Rewards: 79.25581941455124 17.85918745063367\n",
      "6.771087690538025e-09 41.196834564208984 -0.017502789637446448 -10.214969635009766\n",
      "Warning: trajectory cut off by epoch at 49 steps.\n",
      "Rewards: 76.96873030923312 27.30019618078716\n",
      "3.814697446813398e-09 60.688411712646484 -0.011838085107505503 -14.902706146240234\n",
      "Warning: trajectory cut off by epoch at 53 steps.\n",
      "Rewards: 85.52621919130408 23.24126241623662\n",
      "-6.866454960174906e-09 51.340362548828125 -0.015255785199999927 -9.58602523803711\n",
      "Warning: trajectory cut off by epoch at 14 steps.\n",
      "Rewards: 84.61624401280518 21.43487867848889\n",
      "7.629394338515283e-10 35.61873245239258 -0.015094782057404499 -5.432172775268555\n",
      "Warning: trajectory cut off by epoch at 67 steps.\n",
      "Rewards: 84.51239901117347 25.922121831562198\n",
      "7.343292196537732e-09 61.640289306640625 -0.005191760030388792 -8.339515686035156\n",
      "Warning: trajectory cut off by epoch at 38 steps.\n",
      "Rewards: 91.0390849717478 23.440095541422483\n",
      "1.201629640235069e-08 47.41111373901367 -0.01689408989846708 -6.509868621826172\n",
      "Warning: trajectory cut off by epoch at 18 steps.\n",
      "Rewards: 85.37448516180119 26.67445527777029\n",
      "1.3256072683986986e-08 41.82758331298828 -0.014998914891481085 -14.976621627807617\n",
      "Warning: trajectory cut off by epoch at 51 steps.\n",
      "Rewards: 88.1542023482698 18.37429173859163\n",
      "-1.6212463593845428e-09 66.44229125976562 -0.015286117102205732 -29.28304672241211\n",
      "Warning: trajectory cut off by epoch at 32 steps.\n",
      "Rewards: 91.52065770495851 33.0992092738361\n",
      "-1.6307831529616124e-08 52.95764923095703 -0.013031157189606901 -16.121498107910156\n",
      "Warning: trajectory cut off by epoch at 49 steps.\n",
      "Rewards: 89.22387038703891 30.590271162892968\n",
      "1.754760781125242e-08 55.48013687133789 -0.0027268269021067937 -10.099811553955078\n",
      "Warning: trajectory cut off by epoch at 46 steps.\n",
      "Rewards: 87.52227771302918 19.71576972892505\n",
      "-3.8146971692576415e-10 36.17983627319336 -0.014872763855755339 -13.147144317626953\n",
      "Warning: trajectory cut off by epoch at 135 steps.\n",
      "Rewards: 91.79524833023649 23.342767473288962\n",
      "-7.534026735811494e-09 53.884849548339844 -0.015219186349213487 -14.630428314208984\n",
      "Warning: trajectory cut off by epoch at 33 steps.\n",
      "Rewards: 93.35752859959723 28.065698415450125\n",
      "-4.76837147544984e-09 40.062191009521484 -0.017840114310383903 -9.846305847167969\n",
      "Warning: trajectory cut off by epoch at 69 steps.\n",
      "Rewards: 91.35090048606573 25.786581863218558\n",
      "1.907348723406699e-09 42.532554626464844 -0.015216874934733005 -10.157047271728516\n",
      "Warning: trajectory cut off by epoch at 56 steps.\n",
      "Rewards: 99.10650971816808 34.17473874906411\n",
      "-8.201599399626502e-09 36.79921340942383 -0.016688036912679394 -6.855928421020508\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "Rewards: 98.61501380332962 32.767179501549265\n",
      "2.2888184680880386e-09 63.3659553527832 -0.01638902580142032 -9.22232437133789\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "Rewards: 102.50266661672575 29.854421323337284\n",
      "-3.051757735406113e-09 59.410804748535156 -0.016033465716242867 -21.723514556884766\n",
      "Warning: trajectory cut off by epoch at 36 steps.\n",
      "Rewards: 99.57667247633802 33.58864779790744\n",
      "1.2207030941624453e-08 48.5369987487793 -0.009933084349333932 -11.321796417236328\n",
      "Warning: trajectory cut off by epoch at 6 steps.\n",
      "Rewards: 99.52732591119778 34.33584785968497\n",
      "1.4305114870438729e-09 55.20881271362305 -0.005376486141234649 -9.681629180908203\n",
      "Warning: trajectory cut off by epoch at 22 steps.\n",
      "Rewards: 105.8782742570981 31.760738238324905\n",
      "-9.346008411625917e-09 41.26509475708008 -0.016035696458816417 -11.518295288085938\n",
      "Warning: trajectory cut off by epoch at 20 steps.\n",
      "Rewards: 112.21803304367968 42.10312998224975\n",
      "2.145767119543507e-09 39.286991119384766 -0.015092755950987247 -12.026338577270508\n",
      "Warning: trajectory cut off by epoch at 62 steps.\n",
      "Rewards: 110.59778499512215 31.472797088836785\n",
      "3.8146971692576415e-10 98.49711608886719 -0.009843915151059618 -43.62922668457031\n",
      "Warning: trajectory cut off by epoch at 37 steps.\n",
      "Rewards: 102.34449759695346 32.4234115566\n",
      "1.0299682884351569e-08 41.93370819091797 -0.01070689694881466 -13.777738571166992\n",
      "Warning: trajectory cut off by epoch at 135 steps.\n",
      "Rewards: 124.90952753305437 55.2612709966427\n",
      "-1.4686584393075464e-08 86.75769805908203 -0.014754237821698268 -37.3897590637207\n",
      "Warning: trajectory cut off by epoch at 194 steps.\n",
      "Rewards: 112.17776912552739 40.36594660703539\n",
      "-2.6702879907247734e-09 85.89320373535156 -0.011005628977716064 -39.22044372558594\n",
      "Warning: trajectory cut off by epoch at 65 steps.\n",
      "Rewards: 122.69930929476908 59.35604563658082\n",
      "1.277923544762416e-08 59.42070007324219 -0.011450694614648427 -13.161582946777344\n",
      "Warning: trajectory cut off by epoch at 130 steps.\n",
      "Rewards: 108.09009356562679 44.342070228854325\n",
      "-9.536742923144104e-11 124.69711303710938 -0.005637161803990605 -34.46135711669922\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "Rewards: 121.82282265035137 46.50201942344688\n",
      "-2.312660285497259e-08 40.885772705078125 -0.018650994792579922 -8.39923095703125\n",
      "Warning: trajectory cut off by epoch at 41 steps.\n",
      "Rewards: 132.85310437421109 71.95480250495237\n",
      "8.583068700218632e-10 70.18839263916016 -0.005992494916170821 -21.121200561523438\n",
      "Warning: trajectory cut off by epoch at 88 steps.\n",
      "Rewards: 140.9557753686594 80.89179601022991\n",
      "-8.726120270807769e-09 53.54972457885742 -0.010603762818872653 -12.080345153808594\n",
      "Warning: trajectory cut off by epoch at 109 steps.\n",
      "Rewards: 137.01288875766778 87.19258090536331\n",
      "4.577636936176077e-09 182.87594604492188 -0.015179313640296677 -117.38799285888672\n",
      "Warning: trajectory cut off by epoch at 31 steps.\n",
      "Rewards: 171.1903798135545 141.34465996386493\n",
      "9.346008411625917e-09 90.08211517333984 -0.015687685492634884 -19.987503051757812\n",
      "Warning: trajectory cut off by epoch at 82 steps.\n",
      "Rewards: 160.38887100289358 117.17696808877814\n",
      "-8.583068700218632e-10 81.81526184082031 -0.015683061516284957 -23.97711181640625\n",
      "Warning: trajectory cut off by epoch at 771 steps.\n",
      "Rewards: 352.89427779293794 296.55520245488856\n",
      "2.38418573772492e-09 44.755069732666016 -0.014878036044537968 -19.490816116333008\n",
      "Warning: trajectory cut off by epoch at 169 steps.\n",
      "Rewards: 190.79443382265217 183.53594481843518\n",
      "1.7166137400437265e-09 137.50872802734375 -0.019749820086359948 -16.052490234375\n",
      "Warning: trajectory cut off by epoch at 935 steps.\n",
      "Rewards: 177.7913066536114 121.10694790291879\n",
      "1.354217538107605e-08 80.6282730102539 -0.014963507224619477 -26.554035186767578\n",
      "Warning: trajectory cut off by epoch at 29 steps.\n",
      "Rewards: 188.87517787908072 175.2755723331414\n",
      "-6.294250454175199e-09 122.93192291259766 -0.0032269521538168533 -37.16010284423828\n",
      "Warning: trajectory cut off by epoch at 198 steps.\n",
      "Rewards: 174.87885462149404 109.01929989337334\n",
      "-1.239776614703203e-09 68.43196105957031 -0.01768429080843925 -22.489070892333984\n",
      "Warning: trajectory cut off by epoch at 76 steps.\n",
      "Rewards: 256.25087696063906 216.02760562869642\n",
      "-1.201629640235069e-08 187.789794921875 -0.013809485451877102 -36.251129150390625\n",
      "Warning: trajectory cut off by epoch at 468 steps.\n",
      "Rewards: 178.56493907400753 146.958203620912\n",
      "-1.430511542555024e-10 112.1617660522461 -0.016503900146484368 -15.698677062988281\n",
      "Warning: trajectory cut off by epoch at 152 steps.\n",
      "Rewards: 162.204610725435 117.45363910081927\n",
      "-2.0980834847250662e-09 139.9903564453125 -0.015346353638172161 -27.83240509033203\n",
      "Warning: trajectory cut off by epoch at 4 steps.\n",
      "Rewards: 235.0181142716688 185.519650566919\n",
      "3.910064716450279e-09 114.01307678222656 -0.016469212433695812 -46.17950439453125\n",
      "Warning: trajectory cut off by epoch at 92 steps.\n",
      "Rewards: 262.121815114997 236.05968547465844\n",
      "1.4209747156712638e-08 95.88473510742188 -0.016989278846978983 -6.172126770019531\n",
      "Warning: trajectory cut off by epoch at 162 steps.\n",
      "Rewards: 189.7044949582285 143.90744710197703\n",
      "-3.2424927187690855e-09 190.06085205078125 -0.00883685938864942 -31.29937744140625\n",
      "Warning: trajectory cut off by epoch at 393 steps.\n",
      "Rewards: 313.1671830097078 216.52898269100712\n",
      "1.1062621929625038e-08 90.08058166503906 -0.009039458705484726 -47.24053192138672\n",
      "Warning: trajectory cut off by epoch at 478 steps.\n",
      "Rewards: 363.5895318076019 254.51461953113585\n",
      "1.2969970875076342e-08 169.35781860351562 -0.017155496335029774 -88.54307556152344\n",
      "Warning: trajectory cut off by epoch at 634 steps.\n",
      "Rewards: 346.18711807502274 271.29153747278923\n",
      "4.577636936176077e-09 74.56538391113281 -0.012713429439068058 -14.739208221435547\n",
      "Warning: trajectory cut off by epoch at 324 steps.\n",
      "Rewards: 363.13599492299045 318.05326085818837\n",
      "1.4781951662712345e-08 105.82827758789062 -0.017737669751047846 -11.753593444824219\n",
      "Warning: trajectory cut off by epoch at 49 steps.\n",
      "Rewards: 196.04400487938935 177.872051071409\n",
      "-9.727478378351861e-09 103.06746673583984 -0.0142266820698973 -17.610939025878906\n",
      "Warning: trajectory cut off by epoch at 472 steps.\n",
      "Rewards: 350.0892866394762 290.96843173987315\n",
      "5.340575981449547e-09 131.7709197998047 -0.011855649514496136 -82.74169540405273\n",
      "Warning: trajectory cut off by epoch at 283 steps.\n",
      "Rewards: 358.0048079616654 269.35533128850216\n",
      "-2.9754637864698452e-08 90.41874694824219 -0.016845455440879675 -14.298820495605469\n",
      "Warning: trajectory cut off by epoch at 720 steps.\n",
      "Rewards: 432.15125142753016 259.2585602212074\n",
      "-1.850128228397807e-08 73.0123519897461 -0.017171915745734623 -35.18897247314453\n",
      "Warning: trajectory cut off by epoch at 318 steps.\n",
      "Rewards: 371.1190210885354 275.2154597322243\n",
      "6.294250454175199e-09 129.85296630859375 -0.0160613420963287 -8.847129821777344\n",
      "Warning: trajectory cut off by epoch at 324 steps.\n",
      "Rewards: 433.3172327455447 250.70734967285358\n",
      "-6.961822673900997e-09 78.419189453125 -0.012617284920811489 -13.094818115234375\n",
      "Warning: trajectory cut off by epoch at 909 steps.\n",
      "Rewards: 557.4264178538548 207.56484109318615\n",
      "-1.0156631091717827e-08 77.89877319335938 -0.01519559705108442 -9.846931457519531\n",
      "Warning: trajectory cut off by epoch at 397 steps.\n",
      "Rewards: 311.5695546486552 282.230303574575\n",
      "5.7220459481754915e-09 127.34760284423828 -0.018870946153998425 -22.930023193359375\n",
      "Warning: trajectory cut off by epoch at 189 steps.\n",
      "Rewards: 419.59360211225436 291.05108618937726\n",
      "1.0871887390351276e-08 101.66082763671875 -0.018841311031580155 -11.443954467773438\n",
      "Warning: trajectory cut off by epoch at 377 steps.\n",
      "Rewards: 531.3239345893521 273.45568018109327\n",
      "8.77380390562621e-09 98.79151153564453 -0.015944839847088055 -57.877079010009766\n",
      "Warning: trajectory cut off by epoch at 696 steps.\n",
      "Rewards: 308.8757535533314 281.5417477630507\n",
      "9.536742923144104e-11 164.7364501953125 -0.01006752820909023 -63.610565185546875\n",
      "Warning: trajectory cut off by epoch at 197 steps.\n",
      "Rewards: 633.6566502558283 257.92593233252614\n",
      "1.440048258416482e-08 121.25636291503906 -0.015013680292666365 -47.17616271972656\n",
      "Warning: trajectory cut off by epoch at 175 steps.\n",
      "Rewards: 573.0294975864458 264.07381543471524\n",
      "-8.77380390562621e-09 139.4243927001953 -0.0050284085601566275 -51.37620544433594\n",
      "Warning: trajectory cut off by epoch at 611 steps.\n",
      "Rewards: 424.4125033573035 341.3921596611181\n",
      "-6.103515470812226e-09 95.68427276611328 -0.015389531335234796 -7.065086364746094\n",
      "Warning: trajectory cut off by epoch at 350 steps.\n",
      "Rewards: 487.20692668976335 317.9229982476389\n",
      "1.1444092340440193e-09 61.771175384521484 -0.017936949831247384 -11.176155090332031\n",
      "Warning: trajectory cut off by epoch at 24 steps.\n",
      "Rewards: 532.7833237477286 390.5302955410258\n",
      "-1.0824203755532835e-08 124.06568908691406 -0.004755470322072242 -46.235328674316406\n",
      "Warning: trajectory cut off by epoch at 448 steps.\n",
      "Rewards: 439.32986483656896 351.14692883701184\n",
      "2.2888184680880386e-09 225.09315490722656 -0.007946768270433058 -35.02763366699219\n",
      "Warning: trajectory cut off by epoch at 203 steps.\n",
      "Rewards: 743.2178681204914 258.2674052888335\n",
      "-2.6130676289426447e-08 62.77056884765625 -0.011430216899514178 -29.72894287109375\n",
      "Warning: trajectory cut off by epoch at 606 steps.\n",
      "Rewards: 519.5582308265093 368.7594902917499\n",
      "-2.9325486039510906e-08 177.00929260253906 -0.004083100717513943 -51.35887908935547\n",
      "Warning: trajectory cut off by epoch at 25 steps.\n",
      "Rewards: 401.11703953820216 360.1778296064658\n",
      "1.1539459165987864e-08 152.02569580078125 -0.00887247956395143 -23.255126953125\n",
      "Warning: trajectory cut off by epoch at 870 steps.\n",
      "Rewards: 561.2397378436178 323.0285620361868\n",
      "-1.3971328982620435e-08 62.600311279296875 -0.0032681312862781198 -11.136234283447266\n",
      "Warning: trajectory cut off by epoch at 487 steps.\n",
      "Rewards: 701.4273605799896 294.4445612757021\n",
      "5.7220459481754915e-09 27.09214973449707 -0.002963188238441994 -8.939430236816406\n",
      "Warning: trajectory cut off by epoch at 5 steps.\n",
      "Rewards: 786.7657467916338 185.00176012600588\n",
      "3.8146971692576415e-10 28.911155700683594 -0.015333706894516935 -7.665523529052734\n",
      "Warning: trajectory cut off by epoch at 202 steps.\n",
      "Rewards: 332.6041753774411 384.00034171898176\n",
      "-1.0967254659988157e-08 358.9220886230469 -0.010285240806639173 -199.14952087402344\n",
      "Warning: trajectory cut off by epoch at 457 steps.\n",
      "Rewards: 330.4413425951938 247.6202012487939\n",
      "1.2111663671987571e-08 258.4945983886719 -0.018311816272139403 -104.6954345703125\n",
      "Warning: trajectory cut off by epoch at 808 steps.\n",
      "Rewards: 429.63559201515653 271.3734634949751\n",
      "2.7656554379973386e-08 115.8937759399414 -0.015995835602282682 -37.9210205078125\n",
      "Warning: trajectory cut off by epoch at 31 steps.\n",
      "Rewards: 605.0078288211271 416.3603297988852\n",
      "2.2125243859250077e-08 145.49557495117188 -0.01530441805422278 -61.818382263183594\n",
      "Warning: trajectory cut off by epoch at 941 steps.\n",
      "Rewards: 660.8081301206498 311.48931101696553\n",
      "-1.907348590179936e-08 58.163780212402344 -0.016670016124844977 -11.751564025878906\n",
      "Warning: trajectory cut off by epoch at 31 steps.\n",
      "Rewards: 599.7688228277627 337.06661939579715\n",
      "1.3446808111439168e-08 171.74978637695312 -0.0026754384752365468 -33.08479309082031\n",
      "Warning: trajectory cut off by epoch at 311 steps.\n",
      "Rewards: 387.80179390041354 373.131655387403\n",
      "7.629394893626795e-09 163.94827270507812 -0.0192743858247999 -51.24553680419922\n",
      "Warning: trajectory cut off by epoch at 23 steps.\n",
      "Rewards: 808.0749193298575 281.6152439704407\n",
      "9.536743617033494e-10 166.45358276367188 -0.015972922578454063 -43.22135925292969\n",
      "Warning: trajectory cut off by epoch at 147 steps.\n",
      "Rewards: 592.7126135738943 311.35909145349234\n",
      "-1.6212463593845428e-09 99.31385803222656 -0.009249687436223009 -22.41765594482422\n",
      "Warning: trajectory cut off by epoch at 226 steps.\n",
      "Rewards: 464.2571938837561 369.8180714624601\n",
      "-4.1961669694501325e-09 145.1387481689453 -0.012225389941036724 -45.590606689453125\n",
      "Warning: trajectory cut off by epoch at 401 steps.\n",
      "Rewards: 764.4766806541492 310.0610097598675\n",
      "-2.6702881683604573e-08 114.78191375732422 -0.015748623132704864 -70.05748748779297\n",
      "Warning: trajectory cut off by epoch at 437 steps.\n",
      "Rewards: 571.1195046679487 360.2191912754908\n",
      "-9.918212917625624e-09 149.49850463867188 -0.005581251206248972 -18.50762939453125\n",
      "Warning: trajectory cut off by epoch at 78 steps.\n",
      "Rewards: 814.122542260708 185.32265424980045\n",
      "5.722046170220096e-10 69.20378112792969 -0.015562777979671982 -15.516719818115234\n",
      "Warning: trajectory cut off by epoch at 728 steps.\n",
      "Rewards: 617.5248649750389 431.3157047257577\n",
      "-6.675719976811934e-10 35.6434326171875 -0.012154511439800286 -5.185749053955078\n",
      "Warning: trajectory cut off by epoch at 2 steps.\n",
      "Rewards: 724.578669427272 324.0328851519542\n",
      "-1.0299682884351569e-08 60.267555236816406 -0.016338926684856148 -5.1938629150390625\n",
      "Warning: trajectory cut off by epoch at 102 steps.\n",
      "Rewards: 624.2177794877857 417.14241615431894\n",
      "1.1444091896350983e-08 112.7973403930664 -0.017464757233858208 -40.04402160644531\n",
      "Warning: trajectory cut off by epoch at 83 steps.\n",
      "Rewards: 632.3315944041769 330.52273660207317\n",
      "-9.53674295089968e-09 281.1714782714844 -0.006422365792095874 -66.4417724609375\n",
      "Warning: trajectory cut off by epoch at 956 steps.\n",
      "Rewards: 688.3853447533569 360.5835702494842\n",
      "-9.441375681262798e-09 63.84803009033203 -0.007066840643435768 -11.623291015625\n",
      "Warning: trajectory cut off by epoch at 46 steps.\n",
      "Rewards: 386.29284673302266 290.1152252218951\n",
      "7.629394338515283e-10 234.98855590820312 -0.016753976064920406 -56.96636962890625\n",
      "Warning: trajectory cut off by epoch at 942 steps.\n",
      "Rewards: 372.0058422407772 292.5871902798185\n",
      "8.296966669263384e-09 264.3866882324219 -0.017938439649343607 -67.83866882324219\n",
      "Warning: trajectory cut off by epoch at 459 steps.\n",
      "Rewards: 394.3093254072801 360.272361413735\n",
      "1.3732909920349812e-08 70.00244140625 -0.016212421232461693 -19.49683380126953\n",
      "Warning: trajectory cut off by epoch at 16 steps.\n",
      "Rewards: 521.4950508917884 302.31972307636636\n",
      "6.484985437538171e-09 126.34839630126953 -0.015657014179229822 -19.83887481689453\n",
      "Warning: trajectory cut off by epoch at 844 steps.\n",
      "Rewards: 538.7912933683846 419.47820080988083\n",
      "1.2969970875076342e-08 102.04521179199219 -0.013122528722882443 -61.911354064941406\n",
      "Warning: trajectory cut off by epoch at 717 steps.\n",
      "Rewards: 737.7890713383616 270.7376974190128\n",
      "2.403259280470138e-08 157.4527130126953 -0.004033529483526976 -22.41827392578125\n",
      "Warning: trajectory cut off by epoch at 272 steps.\n",
      "Rewards: 273.0107283207846 264.09566162977114\n",
      "6.818771325356465e-09 183.40911865234375 -0.0072971673883497346 -42.93901062011719\n",
      "Warning: trajectory cut off by epoch at 841 steps.\n",
      "Rewards: 460.20737033154603 346.62431768277185\n",
      "-1.525878978725359e-08 197.50015258789062 -0.016694545388221016 -91.84779357910156\n",
      "Warning: trajectory cut off by epoch at 91 steps.\n",
      "Rewards: 515.5760116933855 355.4573523132976\n",
      "1.034736651917001e-08 133.27783203125 -0.016182712382078357 -19.421783447265625\n",
      "Warning: trajectory cut off by epoch at 324 steps.\n",
      "Rewards: 488.9716220982996 365.6194161121416\n",
      "2.317428560161261e-08 97.96340942382812 -0.010791970308124732 -16.711715698242188\n",
      "Warning: trajectory cut off by epoch at 751 steps.\n",
      "Rewards: 504.7200000345078 328.31174207292605\n",
      "9.727478378351861e-09 93.43765258789062 -0.00041207599085790747 -17.100997924804688\n",
      "Warning: trajectory cut off by epoch at 605 steps.\n",
      "Rewards: 413.09860880672653 362.35288126044503\n",
      "1.287460360543946e-08 129.60267639160156 0.0006552925226275264 -27.13646697998047\n",
      "Warning: trajectory cut off by epoch at 607 steps.\n",
      "Rewards: 564.340972853141 305.3728441937586\n",
      "3.528594971768939e-09 99.5798110961914 -0.006831076730787755 -15.742507934570312\n",
      "Warning: trajectory cut off by epoch at 536 steps.\n",
      "Rewards: 873.1707477146141 197.95624174248258\n",
      "1.0013580187262505e-08 29.2738037109375 -0.01659374009072767 -5.628334045410156\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "Rewards: 973.6979651176371 62.15225435960678\n",
      "1.5449524326527353e-08 34.2281494140625 -0.015370909672975941 -12.433477401733398\n",
      "Warning: trajectory cut off by epoch at 81 steps.\n",
      "Rewards: 624.7894307215278 384.1292520248371\n",
      "8.487701208537146e-09 107.59635925292969 -0.015561642231046946 -49.7008171081543\n",
      "Warning: trajectory cut off by epoch at 346 steps.\n",
      "Rewards: 689.9613451167027 414.04425259257397\n",
      "-1.8692016823251834e-08 61.962554931640625 -0.016859151357412117 -25.092090606689453\n",
      "Warning: trajectory cut off by epoch at 699 steps.\n",
      "Rewards: 873.9940294958478 205.13112259733222\n",
      "-7.629394338515283e-10 33.46001434326172 -0.003462716639786978 -5.194927215576172\n",
      "Warning: trajectory cut off by epoch at 98 steps.\n",
      "Rewards: 853.9227092125138 259.2639526996376\n",
      "1.9073485846288207e-10 45.16396713256836 -0.0026541200511157464 -12.670223236083984\n",
      "Warning: trajectory cut off by epoch at 296 steps.\n",
      "Rewards: 799.2492971283464 168.24853120291394\n",
      "2.6702879907247734e-09 113.39887237548828 -0.005895499482005739 -57.6172981262207\n",
      "Warning: trajectory cut off by epoch at 502 steps.\n",
      "Rewards: 661.7445872892905 409.6098162075716\n",
      "-4.768371808516747e-10 61.1735954284668 -0.001522084686905123 -14.100841522216797\n",
      "Warning: trajectory cut off by epoch at 197 steps.\n",
      "Rewards: 551.7930176825079 300.24509462854303\n",
      "-1.206398003716913e-08 146.1461181640625 -0.0008199760191143213 -45.73748779296875\n",
      "Warning: trajectory cut off by epoch at 607 steps.\n",
      "Rewards: 767.3910046363388 241.60906973050973\n",
      "-5.340575981449547e-09 235.50807189941406 -0.004717747928202343 -40.939300537109375\n",
      "Warning: trajectory cut off by epoch at 647 steps.\n",
      "Rewards: 730.376549085538 401.3015464394033\n",
      "-9.441375681262798e-09 147.31585693359375 -0.018339557445049337 -36.39006805419922\n",
      "Warning: trajectory cut off by epoch at 874 steps.\n",
      "Rewards: 706.3800004511833 448.8430010093078\n",
      "-6.675720420901143e-09 35.99927520751953 -0.013670317672192844 -18.015962600708008\n",
      "Warning: trajectory cut off by epoch at 689 steps.\n",
      "Rewards: 579.3683011548152 379.44291810896533\n",
      "-2.813339339269305e-09 123.78227233886719 -0.017524386736750497 -31.162864685058594\n",
      "Warning: trajectory cut off by epoch at 1 steps.\n",
      "Rewards: 576.724757737467 448.01520954520845\n",
      "-1.7738342350526182e-08 88.19703674316406 -0.016625623175501758 -38.05727005004883\n",
      "Warning: trajectory cut off by epoch at 503 steps.\n",
      "Rewards: 678.4994838429553 326.76088370350476\n",
      "2.403259280470138e-08 137.20286560058594 -0.01896665785908702 -35.625152587890625\n",
      "Warning: trajectory cut off by epoch at 563 steps.\n",
      "Rewards: 522.6902201254667 401.2881059057016\n",
      "1.0490417423625331e-09 89.9653091430664 -0.016437611660361284 -28.78216552734375\n",
      "Warning: trajectory cut off by epoch at 935 steps.\n",
      "Rewards: 603.7578034361824 444.346206375063\n",
      "-1.3351439953623867e-09 54.910736083984375 -0.016270073658227968 -6.736202239990234\n",
      "Warning: trajectory cut off by epoch at 993 steps.\n",
      "Rewards: 707.8555528718526 390.57817542333595\n",
      "-4.1961669694501325e-09 91.93405151367188 -0.017111990129947685 -35.233707427978516\n",
      "Warning: trajectory cut off by epoch at 314 steps.\n",
      "Rewards: 632.4260103547128 323.1524479885369\n",
      "6.484985437538171e-09 110.6348648071289 -0.015615256468951788 -56.15532684326172\n",
      "Warning: trajectory cut off by epoch at 441 steps.\n",
      "Rewards: 551.4061638786999 368.58242440303405\n",
      "4.76837147544984e-09 229.531982421875 -0.00800835166126479 -83.85578918457031\n",
      "Warning: trajectory cut off by epoch at 257 steps.\n",
      "Rewards: 586.4560533490251 381.1101916259693\n",
      "-1.2207030941624453e-08 183.677490234375 -0.004028145124018501 -51.144775390625\n",
      "Warning: trajectory cut off by epoch at 198 steps.\n",
      "Rewards: 641.6530707112506 368.61011265550945\n",
      "-1.0871887390351276e-08 65.83159637451172 -0.010830443656444366 -26.521709442138672\n",
      "Warning: trajectory cut off by epoch at 116 steps.\n",
      "Rewards: 516.1303817681476 428.4682525029946\n",
      "-2.241134611224993e-09 72.8544921875 -0.009197466628253492 -42.29301643371582\n",
      "Warning: trajectory cut off by epoch at 873 steps.\n",
      "Rewards: 714.3488514224222 453.30894051498774\n",
      "1.9407272233706863e-08 29.93791961669922 -0.005341243779659166 -7.852176666259766\n",
      "Warning: trajectory cut off by epoch at 512 steps.\n",
      "Rewards: 687.6655798511341 365.4721603976708\n",
      "9.012222079718413e-09 64.66907501220703 -0.015618472059070854 -18.26648712158203\n",
      "Warning: trajectory cut off by epoch at 409 steps.\n",
      "Rewards: 455.9497978325031 430.04529897119403\n",
      "-7.629394893626795e-09 162.04884338378906 -0.0170994157046076 -52.735633850097656\n",
      "Warning: trajectory cut off by epoch at 172 steps.\n",
      "Rewards: 474.5276252785263 449.57821425358753\n",
      "-1.0490417423625331e-08 82.96306610107422 -0.01625250348448759 -45.538875579833984\n",
      "Warning: trajectory cut off by epoch at 671 steps.\n",
      "Rewards: 679.2480049179952 428.9150993805523\n",
      "-1.039505015398845e-08 299.1953125 -0.00772945800647129 -44.22251892089844\n",
      "Warning: trajectory cut off by epoch at 98 steps.\n",
      "Rewards: 908.1866060196529 297.0453388216522\n",
      "3.0994415922691587e-09 212.42527770996094 -0.002590578354895179 -65.2333984375\n",
      "Warning: trajectory cut off by epoch at 72 steps.\n",
      "Rewards: 673.8171118701821 409.36958730741406\n",
      "1.0299682884351569e-08 135.10171508789062 -0.011369443553686409 -89.44844055175781\n",
      "Warning: trajectory cut off by epoch at 917 steps.\n",
      "Rewards: 708.585418706872 343.09676928662407\n",
      "1.7452240541615538e-08 167.01058959960938 -0.0036519691247498542 -32.91099548339844\n",
      "Warning: trajectory cut off by epoch at 269 steps.\n",
      "Rewards: 737.4899799273562 318.55593831601993\n",
      "1.1062621929625038e-08 73.16284942626953 -0.01634331408739076 -18.799705505371094\n",
      "Warning: trajectory cut off by epoch at 582 steps.\n",
      "Rewards: 480.17857377205894 417.29328251269754\n",
      "1.5497207073167374e-08 166.5628662109375 -0.011016796939074425 -92.85269165039062\n",
      "Warning: trajectory cut off by epoch at 178 steps.\n",
      "Rewards: 752.3372011443053 395.07936890346923\n",
      "7.057189943537878e-09 69.75833892822266 -0.007966967837512495 -21.30419921875\n",
      "Warning: trajectory cut off by epoch at 293 steps.\n",
      "Rewards: 889.9947143195614 258.97753309055867\n",
      "1.525878978725359e-08 168.82289123535156 -0.004267443329096565 -38.40043640136719\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "Rewards: 481.8330893041555 345.0941230779903\n",
      "3.814697446813398e-09 202.1441650390625 -0.008406301118433657 -79.58526611328125\n",
      "Warning: trajectory cut off by epoch at 540 steps.\n",
      "Rewards: 644.4281910591926 374.39518752149235\n",
      "7.629394338515283e-10 113.7557144165039 -0.0016537460677325533 -20.27326202392578\n",
      "Warning: trajectory cut off by epoch at 190 steps.\n",
      "Rewards: 771.4913681855318 405.5037934652079\n",
      "-7.152557435219364e-10 73.46414947509766 -0.01571346747875213 -9.573776245117188\n",
      "Warning: trajectory cut off by epoch at 135 steps.\n",
      "Rewards: 614.6669873743507 426.4416161739249\n",
      "-7.1525572131747595e-09 244.58062744140625 -0.01683139830827729 -85.4364013671875\n",
      "Warning: trajectory cut off by epoch at 356 steps.\n",
      "Rewards: 649.5689769989077 402.62555209496503\n",
      "2.3651123726153855e-08 77.37757110595703 -0.0017691309519118903 -11.1571044921875\n",
      "Warning: trajectory cut off by epoch at 20 steps.\n",
      "Rewards: 805.2387207094531 373.28692350684173\n",
      "1.506805347162299e-08 59.092369079589844 -0.0075520744353525515 -9.087623596191406\n",
      "Warning: trajectory cut off by epoch at 827 steps.\n",
      "Rewards: 792.6919509775529 382.40129316208686\n",
      "-1.5640258865801115e-08 56.776161193847656 -0.005811442813277168 -4.9413909912109375\n",
      "Warning: trajectory cut off by epoch at 335 steps.\n",
      "Rewards: 742.8445868948099 294.8132720423186\n",
      "4.482269222449986e-09 281.0089416503906 -0.015062663529813225 -39.25572204589844\n",
      "Warning: trajectory cut off by epoch at 270 steps.\n",
      "Rewards: 657.3092540292517 413.6006853311579\n",
      "-3.509521562250484e-08 162.7139434814453 -0.0054790836915366015 -28.504043579101562\n",
      "Warning: trajectory cut off by epoch at 234 steps.\n",
      "Rewards: 890.3264914061328 382.1844681168858\n",
      "-1.7929076889799944e-08 189.16587829589844 -0.005988063522428533 -45.28559875488281\n",
      "Warning: trajectory cut off by epoch at 788 steps.\n",
      "Rewards: 947.3766881789422 158.21979550515917\n",
      "6.532669072356612e-09 87.26331329345703 -0.005109267322719102 -34.55668258666992\n",
      "Warning: trajectory cut off by epoch at 693 steps.\n",
      "Rewards: 630.4393351923941 363.7173162580323\n",
      "1.0299682884351569e-08 83.48035430908203 -0.00012833995844729884 -15.60296630859375\n",
      "Warning: trajectory cut off by epoch at 139 steps.\n",
      "Rewards: 899.8690791602108 228.74133247652816\n",
      "1.1348724626714102e-08 62.91102600097656 -0.013321897187829279 -5.741458892822266\n",
      "Warning: trajectory cut off by epoch at 308 steps.\n",
      "Rewards: 391.9679960769179 369.70349060514826\n",
      "-3.1471252270875993e-09 207.73728942871094 -0.006456678845733421 -104.45118713378906\n",
      "Warning: trajectory cut off by epoch at 69 steps.\n",
      "Rewards: 929.7189211368955 260.2164845230019\n",
      "5.149841442175784e-09 70.15172576904297 -0.009762121857702866 -4.68609619140625\n",
      "Warning: trajectory cut off by epoch at 531 steps.\n",
      "Rewards: 529.3052816232389 352.2761999064297\n",
      "-1.5640258865801115e-08 136.78140258789062 -0.004573828267306013 -75.42072296142578\n",
      "Warning: trajectory cut off by epoch at 343 steps.\n",
      "Rewards: 595.6163751218646 333.71689762793494\n",
      "-1.201629640235069e-08 125.9551010131836 -0.013979327955842002 -53.64374542236328\n",
      "Warning: trajectory cut off by epoch at 227 steps.\n",
      "Rewards: 1085.734909311415 84.16295070371854\n",
      "2.632141082870021e-08 31.9084415435791 -0.015231275244056874 -18.962547302246094\n",
      "Rewards: 1121.1736286631399 11.109170503765522\n",
      "2.574920721087892e-09 32.59739685058594 -0.014365685884654589 -3.2694778442382812\n",
      "Warning: trajectory cut off by epoch at 528 steps.\n",
      "Rewards: 581.3030869456487 457.11292588084507\n",
      "-7.0095063087194376e-09 176.91064453125 -0.00882973659187547 -89.72006225585938\n",
      "Warning: trajectory cut off by epoch at 510 steps.\n",
      "Rewards: 855.9107724881825 380.85551427323605\n",
      "-2.7656554824062596e-09 37.022789001464844 -0.018163516214489972 -5.865348815917969\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "Rewards: 796.9904420360898 414.0900946777466\n",
      "5.7220459481754915e-09 82.02546691894531 -0.009146318905055573 -34.98981857299805\n",
      "Warning: trajectory cut off by epoch at 713 steps.\n",
      "Rewards: 633.204025473622 422.0065167028818\n",
      "-7.104873578356319e-09 91.11277770996094 -0.0016478787839413478 -12.409980773925781\n",
      "Warning: trajectory cut off by epoch at 279 steps.\n",
      "Rewards: 781.5520701465294 392.84476280874105\n",
      "-1.1444092340440193e-09 111.67636108398438 0.0012837050983682818 -51.564334869384766\n",
      "Warning: trajectory cut off by epoch at 137 steps.\n",
      "Rewards: 813.9292380360993 376.5513711841792\n",
      "-8.416176200398695e-09 69.5980224609375 -0.005256530328839659 -10.745635986328125\n",
      "Warning: trajectory cut off by epoch at 352 steps.\n",
      "Rewards: 783.5939747114566 395.57404002425824\n",
      "1.3351440841802287e-08 102.21727752685547 -0.013098182469606812 -18.11339569091797\n",
      "Warning: trajectory cut off by epoch at 83 steps.\n",
      "Rewards: 638.5987472687017 455.2232566094385\n",
      "6.771087690538025e-09 82.09354400634766 -0.004513474898785397 -15.765548706054688\n",
      "Warning: trajectory cut off by epoch at 3 steps.\n",
      "Rewards: 627.6002118318182 464.71327263261634\n",
      "-2.7656554379973386e-08 71.67167663574219 -0.006939580481500229 -37.762088775634766\n",
      "Warning: trajectory cut off by epoch at 491 steps.\n",
      "Rewards: 582.2768446716005 464.4836712604671\n",
      "-4.768371808516747e-10 38.97349548339844 -0.014918429300188996 -5.351707458496094\n",
      "Warning: trajectory cut off by epoch at 604 steps.\n",
      "Rewards: 416.30665315072935 468.9922699171885\n",
      "2.3555756456516974e-08 124.17976379394531 -0.009737007592619307 -17.214622497558594\n",
      "Warning: trajectory cut off by epoch at 26 steps.\n",
      "Rewards: 730.8594818843089 444.05135377660395\n",
      "-2.00271599304358e-09 97.275634765625 -0.01695231520533569 -35.086639404296875\n",
      "Warning: trajectory cut off by epoch at 403 steps.\n",
      "Rewards: 650.4547030543926 472.2522206178568\n",
      "1.3446808111439168e-08 106.05257415771484 -0.015993571317196142 -71.30572891235352\n",
      "Warning: trajectory cut off by epoch at 661 steps.\n",
      "Rewards: 637.0024517453544 446.20847767539857\n",
      "-1.201629640235069e-08 88.2459716796875 -0.016164261007308944 -15.089561462402344\n",
      "Warning: trajectory cut off by epoch at 228 steps.\n",
      "Rewards: 611.7708876726849 418.402030892756\n",
      "-1.0490417423625331e-09 97.71424102783203 -0.013836173222959047 -30.91710662841797\n",
      "Warning: trajectory cut off by epoch at 483 steps.\n",
      "Rewards: 756.3864733975988 452.4948630541695\n",
      "-2.727508530142586e-08 126.68595886230469 -0.014958884300291686 -80.12339401245117\n",
      "Warning: trajectory cut off by epoch at 569 steps.\n",
      "Rewards: 877.0109431892973 431.39693183903574\n",
      "-1.7929076889799944e-08 79.89115905761719 -0.005811355774104854 -54.11886978149414\n",
      "Warning: trajectory cut off by epoch at 842 steps.\n",
      "Rewards: 625.6878452999786 462.0030928511802\n",
      "3.767013723177115e-08 52.36142349243164 -0.008538404896857088 -8.688831329345703\n",
      "Warning: trajectory cut off by epoch at 144 steps.\n",
      "Rewards: 693.4885384354075 461.3176374444222\n",
      "-4.482269222449986e-09 109.62408447265625 -0.0035798330206424644 -53.82793426513672\n",
      "Warning: trajectory cut off by epoch at 435 steps.\n",
      "Rewards: 658.822930670133 473.62166693682366\n",
      "-4.76837147544984e-09 153.58206176757812 -0.005111113544553625 -27.63146209716797\n",
      "Warning: trajectory cut off by epoch at 866 steps.\n",
      "Rewards: 698.3879809944904 415.2475689679405\n",
      "3.814697446813398e-09 86.546630859375 -0.0005982643072495186 -25.375022888183594\n",
      "Warning: trajectory cut off by epoch at 282 steps.\n",
      "Rewards: 677.9039841519477 479.38119374912776\n",
      "3.2424927187690855e-09 111.4337158203125 -0.007203025260567708 -57.337276458740234\n",
      "Warning: trajectory cut off by epoch at 835 steps.\n",
      "Rewards: 789.0923190523831 480.98395614251444\n",
      "-1.525878978725359e-08 22.188261032104492 -0.020466448381542435 -5.613821029663086\n",
      "Rewards: 694.486157153771 406.4656230115641\n",
      "-6.484985437538171e-09 78.42340087890625 -0.009068248827755365 -25.749107360839844\n",
      "Warning: trajectory cut off by epoch at 384 steps.\n",
      "Rewards: 1083.9132421191525 171.75178627989035\n",
      "-2.4223327343975143e-08 65.72374725341797 -0.0026684412643316335 -25.852951049804688\n",
      "Warning: trajectory cut off by epoch at 146 steps.\n",
      "Rewards: 961.0544269868502 288.9094918797583\n",
      "4.863739189175931e-09 78.56634521484375 -0.0034346144855024185 -9.839797973632812\n",
      "Warning: trajectory cut off by epoch at 154 steps.\n",
      "Rewards: 808.0768391921816 415.3023159400342\n",
      "-8.392333938900265e-09 47.547428131103516 -0.00300424102582042 -11.919902801513672\n",
      "Warning: trajectory cut off by epoch at 453 steps.\n",
      "Rewards: 1046.08360314826 196.71972123750908\n",
      "-1.1062621929625038e-08 43.688926696777344 -0.00370848702415838 -13.208625793457031\n",
      "Warning: trajectory cut off by epoch at 134 steps.\n",
      "Rewards: 950.3454640595246 417.6861012107923\n",
      "-1.4019012617438875e-08 186.43133544921875 -0.0049684936620293385 -48.11402893066406\n",
      "Warning: trajectory cut off by epoch at 5 steps.\n",
      "Rewards: 1161.7717479302514 35.321213059843295\n",
      "8.678436635989328e-09 51.3857307434082 -0.015864937862754225 -16.0997314453125\n",
      "Warning: trajectory cut off by epoch at 576 steps.\n",
      "Rewards: 870.5908131696307 419.74588676754\n",
      "3.733634912350681e-08 57.844913482666016 -0.017501887017488116 -8.927215576171875\n",
      "Warning: trajectory cut off by epoch at 166 steps.\n",
      "Rewards: 943.1294469993173 345.2851689421684\n",
      "-9.536743617033494e-10 50.022823333740234 -0.013462730661034539 -8.5186767578125\n",
      "Warning: trajectory cut off by epoch at 439 steps.\n",
      "Rewards: 597.6252134968713 488.6436951698545\n",
      "-4.673004205812958e-09 211.46878051757812 -0.002521285538002793 -66.28361511230469\n",
      "Warning: trajectory cut off by epoch at 544 steps.\n",
      "Rewards: 607.2387014690726 411.67037613745606\n",
      "4.577636936176077e-09 135.6499481201172 -0.001370498831570366 -11.833045959472656\n",
      "Warning: trajectory cut off by epoch at 431 steps.\n",
      "Rewards: 917.1452149540331 402.16893994339625\n",
      "8.77380390562621e-09 152.70291137695312 -0.008799974267184929 -49.669410705566406\n",
      "Warning: trajectory cut off by epoch at 649 steps.\n",
      "Rewards: 738.536350946554 381.36127913652007\n",
      "1.125335735707722e-08 94.90264129638672 -0.0015764199972156945 -27.098464965820312\n",
      "Warning: trajectory cut off by epoch at 217 steps.\n",
      "Rewards: 722.5911989382143 404.5834680702023\n",
      "1.4114379887075756e-08 165.978759765625 -0.0019988176926970525 -36.79731750488281\n",
      "Warning: trajectory cut off by epoch at 400 steps.\n",
      "Rewards: 905.885371356578 394.50884967633135\n",
      "-1.2969970875076342e-08 39.81296920776367 -0.01977563097476942 -17.335691452026367\n",
      "Warning: trajectory cut off by epoch at 426 steps.\n",
      "Rewards: 1096.1259646744443 142.09338170958088\n",
      "9.155273872352154e-09 37.308265686035156 -0.007631054455787378 -4.0697021484375\n",
      "Warning: trajectory cut off by epoch at 170 steps.\n",
      "Rewards: 963.4573295179931 332.7567682374513\n",
      "-1.1825561863076928e-08 50.44789123535156 -0.016268908327817577 -6.508495330810547\n",
      "Warning: trajectory cut off by epoch at 778 steps.\n",
      "Rewards: 564.7372499328853 481.2906470738066\n",
      "-1.6212463593845428e-09 279.22216796875 -0.003266697990894296 -154.73029327392578\n",
      "Warning: trajectory cut off by epoch at 69 steps.\n",
      "Rewards: 607.7015136955794 437.89367194292043\n",
      "3.433227480087453e-09 156.66796875 -0.003355921885371149 -34.01332092285156\n",
      "Warning: trajectory cut off by epoch at 34 steps.\n",
      "Rewards: 857.7913645260844 428.7194033416975\n",
      "-6.008148201175345e-09 88.1687240600586 -0.008205425505340092 -17.731277465820312\n",
      "Warning: trajectory cut off by epoch at 216 steps.\n",
      "Rewards: 1148.2820306680521 88.20142917046064\n",
      "-3.6239624634504253e-09 34.69303894042969 -0.01704609217941755 -5.628154754638672\n",
      "Warning: trajectory cut off by epoch at 897 steps.\n",
      "Rewards: 983.4577521475443 416.6386033942181\n",
      "1.1062621929625038e-08 10.062996864318848 -0.00027746067233369587 -3.8356642723083496\n",
      "Warning: trajectory cut off by epoch at 152 steps.\n",
      "Rewards: 850.7257878609842 338.2829720569066\n",
      "1.1777878228258487e-08 85.02616882324219 -0.004092594279349271 -23.797500610351562\n",
      "Warning: trajectory cut off by epoch at 167 steps.\n",
      "Rewards: 729.8864867432952 539.0983642106958\n",
      "1.1920929132713809e-08 183.32034301757812 -0.003446642821654855 -86.17572784423828\n",
      "Warning: trajectory cut off by epoch at 125 steps.\n",
      "Rewards: 747.7886435917399 484.6606031438706\n",
      "-2.632141082870021e-08 119.3213119506836 -0.004190774855763024 -40.779449462890625\n",
      "Warning: trajectory cut off by epoch at 108 steps.\n",
      "Rewards: 746.2217820568939 526.0231297345753\n",
      "2.861023085110048e-10 131.83349609375 -0.005594955577701344 -85.87097549438477\n",
      "Warning: trajectory cut off by epoch at 998 steps.\n",
      "Rewards: 618.5431545373558 408.0530104531597\n",
      "1.1062621929625038e-08 132.08770751953125 -0.0013235631663350844 -35.51146697998047\n",
      "Warning: trajectory cut off by epoch at 277 steps.\n",
      "Rewards: 648.7004824934666 527.4752019598161\n",
      "3.2424927187690855e-09 140.9235076904297 -0.0026559033259749842 -30.57292938232422\n",
      "Warning: trajectory cut off by epoch at 554 steps.\n",
      "Rewards: 607.3412217262506 481.67121563268245\n",
      "3.337859988405967e-10 202.18603515625 -0.007979574947059143 -78.85935974121094\n",
      "Warning: trajectory cut off by epoch at 196 steps.\n",
      "Rewards: 543.9878273439682 441.1348000455408\n",
      "6.961822673900997e-09 238.4073486328125 -0.007028564321994946 -79.63168334960938\n",
      "Warning: trajectory cut off by epoch at 32 steps.\n",
      "Rewards: 752.7851782255876 476.1357210942357\n",
      "-1.2207030941624453e-08 60.978363037109375 -0.0107949943214658 -20.10715103149414\n",
      "Warning: trajectory cut off by epoch at 427 steps.\n",
      "Rewards: 698.1203048500977 516.8467001114301\n",
      "-2.3651123726153855e-08 221.5923309326172 -0.013522415985166347 -44.11677551269531\n",
      "Warning: trajectory cut off by epoch at 76 steps.\n",
      "Rewards: 591.793057132599 460.2787268565245\n",
      "-2.479553229406406e-09 144.70230102539062 -0.008393368998169892 -36.964927673339844\n",
      "Warning: trajectory cut off by epoch at 41 steps.\n",
      "Rewards: 674.1541200093719 422.9381974587685\n",
      "-3.4046173880142305e-08 109.28684997558594 -0.017297479736804178 -23.49365997314453\n",
      "Warning: trajectory cut off by epoch at 367 steps.\n",
      "Rewards: 629.6437796703273 536.09819658092\n",
      "6.294250454175199e-09 97.7210693359375 -0.017196276000142063 -39.214900970458984\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Rewards: 869.6733771601413 333.6638314115441\n",
      "6.580352707175052e-09 230.69529724121094 -0.005537270629405899 -45.278961181640625\n",
      "Warning: trajectory cut off by epoch at 118 steps.\n",
      "Rewards: 747.2492386832378 487.4539747232865\n",
      "3.814697446813398e-09 118.76973724365234 -0.012668417856097403 -70.16781616210938\n",
      "Warning: trajectory cut off by epoch at 182 steps.\n",
      "Rewards: 733.8992081095658 514.9679943348773\n",
      "-8.916854810081531e-09 197.7040252685547 -0.0028786986839026696 -139.77495193481445\n",
      "Warning: trajectory cut off by epoch at 216 steps.\n",
      "Rewards: 718.8442385618727 524.7552910333793\n",
      "7.1525572131747595e-09 59.68840026855469 -0.01873495213687404 -19.82434844970703\n",
      "Warning: trajectory cut off by epoch at 811 steps.\n",
      "Rewards: 734.7079083133011 565.9862585425986\n",
      "-3.433227480087453e-09 143.88279724121094 -0.011491946236789286 -67.09806823730469\n",
      "Warning: trajectory cut off by epoch at 613 steps.\n",
      "Rewards: 670.5019904564942 383.80543942917234\n",
      "-3.776550272505119e-08 229.94204711914062 -0.005588303738832678 -87.438720703125\n",
      "Warning: trajectory cut off by epoch at 598 steps.\n",
      "Rewards: 551.0795863139425 360.8533348008562\n",
      "8.869171175263091e-09 182.90611267089844 -0.0026242521341890424 -61.10900115966797\n",
      "Warning: trajectory cut off by epoch at 501 steps.\n",
      "Rewards: 778.2063114039247 514.8754540871042\n",
      "6.675719976811934e-10 223.2022247314453 -0.004547377784550166 -56.58125305175781\n",
      "Warning: trajectory cut off by epoch at 109 steps.\n",
      "Rewards: 845.8163790479628 466.13613985123976\n",
      "1.0013580187262505e-08 272.7022399902344 -0.015134949937462672 -61.845184326171875\n",
      "Warning: trajectory cut off by epoch at 136 steps.\n",
      "Rewards: 678.9398815879364 401.84658304946055\n",
      "9.346008411625917e-09 136.5648651123047 -0.010302517595887295 -56.28349304199219\n",
      "Warning: trajectory cut off by epoch at 130 steps.\n",
      "Rewards: 839.3225666225869 520.6998250261755\n",
      "-2.3698806472793876e-08 206.29466247558594 -0.004385567320138506 -109.36775970458984\n",
      "Warning: trajectory cut off by epoch at 456 steps.\n",
      "Rewards: 808.0338587148764 528.3787816151493\n",
      "2.1934509319976314e-08 102.8102035522461 -0.009583168245852036 -28.45111083984375\n",
      "Warning: trajectory cut off by epoch at 98 steps.\n",
      "Rewards: 612.8594106569255 462.0996779184255\n",
      "3.814697446813398e-09 375.6080017089844 -0.0017324078092353368 -80.05130004882812\n",
      "Warning: trajectory cut off by epoch at 296 steps.\n",
      "Rewards: 725.2908913365277 514.023305855551\n",
      "-2.9373168786150927e-08 58.059505462646484 -0.005113554150611321 -14.041767120361328\n",
      "Warning: trajectory cut off by epoch at 710 steps.\n",
      "Rewards: 750.2959758373537 453.71540018776494\n",
      "-1.92165376944331e-08 164.54058837890625 -0.00181562976576366 -67.8156509399414\n",
      "Warning: trajectory cut off by epoch at 833 steps.\n",
      "Rewards: 851.5439517594114 434.6362276277687\n",
      "1.2922287240257901e-08 45.858184814453125 -0.0025842768374833014 -10.147247314453125\n",
      "Warning: trajectory cut off by epoch at 369 steps.\n",
      "Rewards: 796.4919841140719 474.9871823047681\n",
      "3.3378602104505717e-09 54.48439407348633 -0.018675667777657612 -8.078998565673828\n",
      "Rewards: 1198.5760138178744 30.85056680789669\n",
      "1.7929076889799944e-08 13.190990447998047 -0.017877451378106812 -4.543571472167969\n"
     ]
    }
   ],
   "source": [
    "env_fn = lambda : gym.make('Walker2DBulletEnv-v0')\n",
    "\n",
    "ac_kwargs = dict(hidden_sizes=[64,64])\n",
    "\n",
    "\n",
    "ppo(env_fn=env_fn, ac_kwargs=ac_kwargs, steps_per_epoch=5000, epochs=250, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bad11-5bb3-4407-88c4-a5af59b45716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
